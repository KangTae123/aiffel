{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rock scissor paper classification\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "img_path = os.getenv('HOME') + '/aiffel/data/e1_data/rcp/train/'\n",
    "\n",
    "target_size = (28,28)\n",
    "\n",
    "rock_len = 0\n",
    "scissor_len = 0\n",
    "paper_len = 0\n",
    "\n",
    "for num in range(1,42):\n",
    "    for folder in ['/rock','/scissor','/paper'] :\n",
    "        images = glob.glob(img_path+str(num)+folder+'/*.jpg')\n",
    "        #print(folder)\n",
    "        #print(len(images))\n",
    "        \n",
    "        if folder == '/rock':\n",
    "            rock_len += len(images)\n",
    "        elif folder == '/scissor':\n",
    "            scissor_len += len(images)\n",
    "            #print(len(images))\n",
    "        else:\n",
    "            paper_len += len(images)\n",
    "            \n",
    "        #if len(images)!=100:\n",
    "        #    print(img_path+str(num)+folder, len(images))\n",
    "        \n",
    "        for img in images:\n",
    "            old_img = Image.open(img)\n",
    "            new_img = old_img.resize(target_size,Image.ANTIALIAS)\n",
    "            new_img.save(img,'JPEG')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### image들은 36명의 각자의 묵찌빠 사진을 취합하였다\n",
    "#### 각 사람들의 image는 train dir과 test dir로 나뉘어 1~41 의 이름을 가진 dir에 있으며 \n",
    "#### 각 사람의 dir안에는 rock paper sccissor의 directory로 나뉘어 image가 들어가있다\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 각 directory별로 loop돌면서 모든 image들을 28x28로 resize 시켜주었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rock_len :  2708\n",
      "scissor_len :  2711\n",
      "paper_len :  2704\n",
      "총 이미지 개수 :  8123\n"
     ]
    }
   ],
   "source": [
    "print('rock_len : ', rock_len)\n",
    "print('scissor_len : ', scissor_len)\n",
    "print('paper_len : ', paper_len)\n",
    "\n",
    "total_img_len = rock_len + scissor_len + paper_len\n",
    "\n",
    "print('총 이미지 개수 : ', total_img_len)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 가위, 바위, 보 별 이미지 갯수는 위에 나와있는 것과 같으며 총 이미지 갯수는 10523개 이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#image check\n",
    "images = glob.glob(img_path+str(1)+'/rock'+'/*.jpg')\n",
    "\n",
    "img = Image.open(images[1])\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8424, 28, 28, 3)\n",
      "(8424,)\n",
      "학습데이터(x_train)의 이미지 개수는 8123 입니다.\n",
      "x_train shape: (8424, 28, 28, 3)\n",
      "y_train shape: (8424,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "img_size=28\n",
    "color=3\n",
    "train_image_size = 8424\n",
    "\n",
    "train_imgs=np.zeros(train_image_size*img_size*img_size*color,dtype=np.int32).reshape(train_image_size,img_size,img_size,color)\n",
    "train_labels=np.zeros(train_image_size,dtype=np.int32)\n",
    "\n",
    "print(train_imgs.shape)\n",
    "print(train_labels.shape)\n",
    "\n",
    "idx=0\n",
    "\n",
    "\n",
    "\n",
    "for num in range(1,33):\n",
    "    for file in glob.glob(img_path+str(num)+'/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        train_imgs[idx,:,:,:]=img    \n",
    "        train_labels[idx]=0   # rock : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.glob(img_path+str(num)+'/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        train_imgs[idx,:,:,:]=img    \n",
    "        train_labels[idx]=1   # scissor : 1\n",
    "        idx=idx+1\n",
    "    for file in glob.glob(img_path+str(num)+'/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        train_imgs[idx,:,:,:]=img   \n",
    "        train_labels[idx]=2   # paper : 2\n",
    "        idx=idx+1\n",
    "    \n",
    "print(\"학습데이터(x_train)의 이미지 개수는\",idx,\"입니다.\")\n",
    "\n",
    "x_train_norm = train_imgs/255.0\n",
    "\n",
    "print(\"x_train shape: {}\".format(x_train_norm.shape))\n",
    "print(\"y_train shape: {}\".format(train_labels.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test set을 위한(10523, 28, 28, 3) 의 shape를 가진 ndarray를 0으로 initiallize시켜준다\n",
    "#### training set을 위한 ndarray도 initialize시켜준다\n",
    "#### 각 rock, scissor, paper 로 분류된 directory 들을 돌며 image들을 test set에 넣고 directory이름을 통해 label도 값을 넣어준다\n",
    "#### 각 pixel 은 channel당 0-255의 값을 갖기에 0-1의값을 갖도록 normalize 시켜준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8423, 28, 28, 3)\n",
      "(1, 28, 28, 3)\n",
      "(8423,)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_tr, x_val, y_tr, y_val = train_test_split(x_train_norm, train_labels,test_size=0.00001, shuffle=True)\n",
    "\n",
    "print(x_tr.shape)\n",
    "print(x_val.shape)\n",
    "print(y_tr.shape)\n",
    "print(y_val.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모든 dataset를 8:2의 비율로 training data와 test data로 split해주었다가 잘못되었음을 느끼고 다시 위에서 train test set 으로 나눔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 26, 26, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               204928    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 224,707\n",
      "Trainable params: 224,707\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 6739 samples\n",
      "Epoch 1/5\n",
      "6739/6739 [==============================] - 1s 83us/sample - loss: 0.8413 - accuracy: 0.5982\n",
      "Epoch 2/5\n",
      "6739/6739 [==============================] - 0s 45us/sample - loss: 0.4963 - accuracy: 0.8050\n",
      "Epoch 3/5\n",
      "6739/6739 [==============================] - 0s 47us/sample - loss: 0.3181 - accuracy: 0.8817\n",
      "Epoch 4/5\n",
      "6739/6739 [==============================] - 0s 49us/sample - loss: 0.2220 - accuracy: 0.9211\n",
      "Epoch 5/5\n",
      "6739/6739 [==============================] - 0s 44us/sample - loss: 0.1616 - accuracy: 0.9448\n",
      "1685/1685 [==============================] - 0s 74us/sample - loss: 0.1641 - accuracy: 0.9412\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 26, 26, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               204928    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 224,707\n",
      "Trainable params: 224,707\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 6739 samples\n",
      "Epoch 1/5\n",
      "6739/6739 [==============================] - 0s 70us/sample - loss: 0.8761 - accuracy: 0.5732\n",
      "Epoch 2/5\n",
      "6739/6739 [==============================] - 0s 45us/sample - loss: 0.5288 - accuracy: 0.7820\n",
      "Epoch 3/5\n",
      "6739/6739 [==============================] - 0s 46us/sample - loss: 0.3354 - accuracy: 0.8734\n",
      "Epoch 4/5\n",
      "6739/6739 [==============================] - 0s 45us/sample - loss: 0.2254 - accuracy: 0.9165\n",
      "Epoch 5/5\n",
      "6739/6739 [==============================] - 0s 48us/sample - loss: 0.1546 - accuracy: 0.9467\n",
      "1685/1685 [==============================] - 0s 69us/sample - loss: 0.2066 - accuracy: 0.9306\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 26, 26, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               204928    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 224,707\n",
      "Trainable params: 224,707\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 6739 samples\n",
      "Epoch 1/5\n",
      "6739/6739 [==============================] - 1s 77us/sample - loss: 0.8784 - accuracy: 0.5652\n",
      "Epoch 2/5\n",
      "6739/6739 [==============================] - 0s 55us/sample - loss: 0.5429 - accuracy: 0.7753\n",
      "Epoch 3/5\n",
      "6739/6739 [==============================] - 0s 46us/sample - loss: 0.3356 - accuracy: 0.8740\n",
      "Epoch 4/5\n",
      "6739/6739 [==============================] - 0s 49us/sample - loss: 0.2257 - accuracy: 0.9159\n",
      "Epoch 5/5\n",
      "6739/6739 [==============================] - 0s 49us/sample - loss: 0.1532 - accuracy: 0.9472\n",
      "1685/1685 [==============================] - 0s 64us/sample - loss: 0.1764 - accuracy: 0.9359\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 26, 26, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               204928    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 224,707\n",
      "Trainable params: 224,707\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 6739 samples\n",
      "Epoch 1/5\n",
      "6739/6739 [==============================] - 0s 72us/sample - loss: 0.8546 - accuracy: 0.5741\n",
      "Epoch 2/5\n",
      "6739/6739 [==============================] - 0s 44us/sample - loss: 0.4829 - accuracy: 0.8080\n",
      "Epoch 3/5\n",
      "6739/6739 [==============================] - 0s 45us/sample - loss: 0.3141 - accuracy: 0.8804\n",
      "Epoch 4/5\n",
      "6739/6739 [==============================] - 0s 44us/sample - loss: 0.2156 - accuracy: 0.9228\n",
      "Epoch 5/5\n",
      "6739/6739 [==============================] - 0s 44us/sample - loss: 0.1505 - accuracy: 0.9473\n",
      "1685/1685 [==============================] - 0s 59us/sample - loss: 0.1633 - accuracy: 0.9436\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 26, 26, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               204928    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 224,707\n",
      "Trainable params: 224,707\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6740 samples\n",
      "Epoch 1/5\n",
      "6740/6740 [==============================] - 1s 89us/sample - loss: 0.8469 - accuracy: 0.5904\n",
      "Epoch 2/5\n",
      "6740/6740 [==============================] - 0s 44us/sample - loss: 0.4769 - accuracy: 0.8015\n",
      "Epoch 3/5\n",
      "6740/6740 [==============================] - 0s 44us/sample - loss: 0.3087 - accuracy: 0.8797\n",
      "Epoch 4/5\n",
      "6740/6740 [==============================] - 0s 44us/sample - loss: 0.2123 - accuracy: 0.9233\n",
      "Epoch 5/5\n",
      "6740/6740 [==============================] - 0s 43us/sample - loss: 0.1382 - accuracy: 0.9513\n",
      "1684/1684 [==============================] - 0s 61us/sample - loss: 0.1633 - accuracy: 0.9460\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# 모델 시험\\ntest_loss, test_accuracy = model2.evaluate(x_val, y_val, verbose=2)\\nprint(\"test_loss: {} \".format(test_loss))\\nprint(\"test_accuracy: {}\".format(test_accuracy))\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "n_channel_1 = 32\n",
    "n_channel_2 = 64\n",
    "n_dense = 128\n",
    "n_train_epoch=30\n",
    "\n",
    "def create_model():\n",
    "    model=keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(n_channel_1, (3,3), activation='relu', input_shape=(28,28,3)))\n",
    "    model.add(keras.layers.MaxPool2D(2,2))\n",
    "    model.add(keras.layers.Conv2D(n_channel_2, (3,3), activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(n_dense, activation='relu'))\n",
    "    model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, epochs=5)\n",
    "\n",
    "score = cross_val_score(model, x_train_norm, train_labels, cv=kfold, n_jobs=1)\n",
    "\n",
    "# 모델 훈련\n",
    "#model.fit(x_tr, y_tr, epochs=n_train_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.94124627, 0.93056381, 0.93590504, 0.9436202 , 0.94596201])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross validation score이 의미가없다.\n",
    "## train test split도 의미가 없는데...\n",
    "## 사실 약 40명분당 가위바위보 별 100장씩의 image를  갖고있지만 사실 한명당 3장의 가위바위보에 약간의 노이즈를 추가시킨 99장의 image라고 생각한다\n",
    "## 더이상 내가아는 검증 방법은없다\n",
    "## 같은 구조의 모델이 왜 같은 데이터와 같은 파라미터로 학습시켰는데 왜  모델마다 같은 test set에대한 성능차이의 폭이 큰지 모르겠다\n",
    "## 더이상은 이 프로젝트의 범위를 넘어가는 것 같아 \n",
    "## 그냥 일단 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터의 이미지 개수는 2100 입니다.\n"
     ]
    }
   ],
   "source": [
    "number_of_data=2100\n",
    "img_size=28\n",
    "color=3\n",
    "\n",
    "test_imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "test_labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "img_path = os.getenv('HOME') + '/aiffel/data/e1_data/rcp/test/'\n",
    "\n",
    "idx=0\n",
    "\n",
    "for num in range(33,42):\n",
    "    for file in glob.glob(img_path+str(num)+'/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        test_imgs[idx,:,:,:]=img    \n",
    "        test_labels[idx]=0   # rock : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.glob(img_path+str(num)+'/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        test_imgs[idx,:,:,:]=img    \n",
    "        test_labels[idx]=1   # scissor : 1\n",
    "        idx=idx+1\n",
    "    for file in glob.glob(img_path+str(num)+'/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        test_imgs[idx,:,:,:]=img   \n",
    "        test_labels[idx]=2   # paper : 2\n",
    "        idx=idx+1\n",
    "    \n",
    "print(\"학습데이터의 이미지 개수는\",idx,\"입니다.\")\n",
    "\n",
    "x_test_norm = test_imgs/255.0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100/2100 - 0s - loss: 3.0854 - accuracy: 0.6995\n",
      "test_loss: 3.0853689590476763 \n",
      "test_accuracy: 0.6995238065719604\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test_norm, test_labels, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
